---
title: "Credit Card Brand Attribute Ratings"
author: "Mark Preston"
date: "November 26, 2018"
output: 
  html_document: 
    fig_height: 6.5
    fig_width: 10.5
---

***

###Introduction: Examining brand attribute ratings and consumer preferences

In this analysis, I'll use consumer survey data collected on individual preferences towards credit card companies. This brand strategy data provides a forum for consumers to provide feedback on how they feel about certain banking products. Overall, this analysis will work towards buillding models that establish a relationship between the brand attribute ratings consumers provided and their preference ratings for credit cards.

Before proceeding with the work, I'll highlight data transformations which I performed. Most notably, the initial data set has unclear, non-descriptive column names. The columns are the primarily comprised of the brand attribute ratings so I felt it was essential to better label these. As such, I've provided more descriptive column names. Beyond this, I've changed the missing data values from -99 to 0 (after altering the zero values for the pri_sec column for consistency) and added the brand names for each credit card.

```{r loading data and packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(corrplot)
library(lavaan)
library(semPlot)
library(Metrics)
library(kableExtra)
library(knitr)

#setting ggplot preference
theme_set(
  theme_minimal()
)

#custom table function
custom_kable <- function(x){
  kable(x, format = "html") %>%
    kable_styling(bootstrap_options = "striped")
}

cc_brands <- read.csv("Brand_stratategy_data.csv") %>%
  rename(respondent = "Ã¯..respid",
         revenue_group = hq_cell,
         brand = brandid,
         comp_pref = q6,
         preference_rating = consider,
         easy_access = att_rating1,
         channel_access = att_rating2,
         products_for_bus = att_rating3,
         bus_cash_flow = att_rating4,
         cool = att_rating5,
         emerging_leader = att_rating6,
         innovative = att_rating7,
         personalize_service = att_rating8,
         trustworthy = att_rating9,
         responsive_bus_needs = att_rating10,
         business_advisor = att_rating11,
         understand_bus_probs = att_rating12,
         solves_bus_probs = att_rating13,
         my_best_interests = att_rating14,
         prestige = att_rating15,
         feel_valued = att_rating16,
         dual_recognition = att_rating17,
         rewards_for_good = att_rating18,
         bus_rewards_program = att_rating19,
         easy_application = att_rating20,
         customer_service = att_rating21,
         easy_bus_finance = att_rating22,
         clear_comms = att_rating23,
         account_infosec = att_rating24,
         life_easier = att_rating25,
         easy_bus_invest = att_rating26,
         quick_app_approval = att_rating27,
         reasonable_fees = att_rating28,
         best_prod_rates = att_rating29,
         fair_treatment = att_rating30,
         good_value = att_rating31) %>%
  mutate(pri_sec = ifelse(pri_sec == 0 , 3, pri_sec)) %>%
  mutate_all(function(x) ifelse(x == -99, 0, x)) %>%
  mutate(brand = case_when(
    brand == 1 ~ "Advanta",
    brand == 2 ~ "Amex",
    brand == 3 ~ "BoA",
    brand == 4 ~ "Chase",
    brand == 5 ~ "Citibank",
    brand == 6 ~ "HSBC",
    brand == 7 ~ "Wells_Fargo",
    brand == 8 ~ "Other_prim",
    brand == 9 ~ "Other_sec"
  )
)
```

***

###Exploratory Data Anlaysis: Reviewing credit card preferences

To start, I want to review how many responses each individual has. A count table for the first five respondents shows 9 responses each, or one for each brand.

```{r number of customer reviews}
cc_brands %>%
  count(respondent) %>%
  head() %>%
  kable(format = "html", align = "l") %>%
  kable_styling(bootstrap_options = "striped")
```

As a data quality check, I also wanted to make sure every respondent had nine brand reviews. The logical output shows that there are no respondents without the required number. This means no further data cleaning needs to take place.

```{r checking to see if consumers have equal values}
cc_brands %>%
  count(respondent) %>%
  summarise(any_respondent_without_nine_reviews = any(n != 9)) %>%
  custom_kable()
```

Moving forward, I also wanted to review how many respondents fall into each of the revenue groups. There is essentially a low, medium, and high group for this variable; of these, high has the most participants.

```{r revenue category counts}
cc_brands %>%
  filter(!duplicated(respondent)) %>%
  count(revenue_group) %>%
  kable(format = "html", align = "l") %>%
  kable_styling(bootstrap_options = "striped")
```

Getting into the cardholder details, I've developed a table with the primary card counts. the largest group is respondents indicating they use a primary card outside of the seven named brands. After this, Bank of America has the highest count. Advanta has the fewest respondents using it as a primary card with only two. These are the first indications of how different brands might be perceived by consumers. For example, I do not think Advanta will have especially high ratings given the low primary usage.

```{r reviewing primary card choice}
cc_brands %>%
  filter(pri_sec == 1) %>%
  count(pri_sec, brand, sort = T) %>%
  kable(format = "html", align = "l") %>%
  kable_styling(bootstrap_options = "striped")
```

Building on the previous section, I also wanted to review missing data for each brand. The initial data set contains numerous missing values, which are present when a respondent doesn't know the brand and cannot offer a brand rating. In essence, I think this becomes a proxy for brand awareness where a low number of missing values means more respondents have an opinion or direct association with the card. As seen, Advanta has the most missing values, which is in line with the previous section where it had the fewest primary users. Amex and Chase have the fewest, which seems intuitive given how large the brands are. 

```{r checking on missing data}
cc_brands %>%
  select(3, 8:38) %>%
  group_by(brand) %>%
  gather(key = "variable", value = "value", -brand) %>%
  filter(value == 0) %>%
  count(value, sort = T) %>%
  custom_kable()
```

Moving into the preference ratings, I've put together the average score for each brand. The highest mean score for a named brand is Amex while an unnamed secondary card- which could be more than one brand- is top here. The previous work seemed to indicate Advanta might have a low rating, which is confirmed here; it has the lowest average preference rating by about 2.7 points. The rating scale here is from 1 to 11 (and I've filtered out the unknown values, which are 12) so that means Advanta isn't much above the lowest score.

```{r avg perference rating}
cc_brands %>%
  filter(preference_rating != 12) %>%
  group_by(brand) %>%
  summarise(avg_pref_rating = mean(preference_rating)) %>%
  arrange(desc(avg_pref_rating)) %>%
  custom_kable()
```

As a final review before the modelling phase, I wanted to see the average brand rating given by respondents across each question and brand. Amex sticks out here again as a brand with high consumer ratings. For example, respondents gave the question about information security an average of about 2.5. On a scale with only 3 values, this means that respondents view the score at about medium-high on average. Chase and Bank of America also stick out as having all average scores over 1.5. Yet again, Advanta shows the poorest metrics of any card, this time with the lowest brand ratings. The crucial question moving forward is how these brand ratings are associated with the consumer prefererence ratings.

```{r avg brand rating by brand and question, fig.height=10, fig.width=11}
cc_brands %>%
  select(3, 8:38) %>%
  group_by(brand) %>%
  summarise_if(is.numeric, function(x) mean(x)) %>%
  gather(key = "variable", value = "value", -brand) %>%
  mutate(brand = factor(brand, levels = c("Other_sec", "Amex", "Citibank", "BoA",
                                          "Chase", "Wells_Fargo", "HSBC",
                                          "Other_prim", "Advanta")),
         variable = reorder(variable, value)) %>%
  filter(value != 4) %>%
  ggplot(aes(variable, value, colour = brand, fill = brand)) +
  geom_col(show.legend = F) +
  geom_hline(yintercept = 1.5, colour = "firebrick1", size = 1.3, alpha = .75) +
  coord_flip() +
  facet_wrap(facets = "brand") +
  labs(title = "Average brand rating by question in credit card preference set",
       subtitle = "Amex and Chase appear to have highest brand ratings (values scored from 1 to 3); red line denotes middle score (1.5)",
       x = NULL,
       y = "brand rating avg")
```

***

###Linear model with brand ratings: Structural problems and multicollinearity

One method for assessing the relationship between brand ratings and preference is regression. However, this approach is difficult owing to a few issues. First, the brand rating data are generally quite noisy owing to a small scale (1 to 4) which can create a lot of variance between respondent answers. 

Additionally, the set has extremely high multicollinearity. The correlation plot below shows that each variable has very high correltion with almost every other variable. Again, this is largely because all the values are positive and closely aligned to begin with, which inherently introduces this structure. The introduction of multicollinearity inflates standard errors and therefore, will negatively impact any regression modelling.

```{r correlation plot between variables}
cc_brands %>%
  select(7:38) %>%
  cor() %>%
  corrplot(type = "upper")
```

Just to highlight this, I've developed a linear model with all the brand ratings. The plot below shows the predictions are erratic and at times nowhere near correct. Overall, this would not be suitable for any prediction task.

```{r modelling set up}
profiles <- c(seq(1, nrow(cc_brands), 9), seq(5, nrow(cc_brands), 9)) %>%
  as.data.frame() %>%
  rename(holdout = ".") %>%
  arrange(holdout)

brand_train <- cc_brands %>%
  select(1, 7:38) %>%
  filter(!row_number() %in% profiles$holdout)

brand_test <- cc_brands %>%
  select(1, 7:38) %>%
  filter(row_number() %in% profiles$holdout)

brand_lm <- lm(preference_rating ~ ., data = brand_train[,-1])

data.frame(
  record = seq(1, nrow(brand_train), 1),
  fitted = brand_lm$fitted.values,
  actual = brand_train$preference_rating
) %>% 
  mutate(actual = ifelse(actual == 0, .1, actual),
         percent_diff = (actual - fitted) / actual * 100) %>%
  ggplot(aes(record, percent_diff)) +
  geom_line(colour = "dodgerblue2", alpha = .5) +
  labs(title = "Percent differences between actuals and fitted values: (act - fit) / fit",
       subtitle = "Model shows a very poor fit owing in part to multicollinearity")
```

Apart from the predictive component, the model summary also provides cause for concern. The slope coefficients and p-values are variable and do not show any intuitive pattern for why a feature might be positive or negative. Again, this is the product of multicollinearity. Overall, I do not think this approach yields a reasonable solution  

```{r brand lm summary}
summary(brand_lm)
```

***

###PCA regression and factor analysis with brand data

To effectively deal with the set's multicollinearity, principle components analysis (PCA) is a reasonable solution. The method orthogonalizes the input data resulting in predictors that are uncorrelated. Additionally, the information from numerous columns can be compacted into fewer variables so there are fewer variables to work with (dimensional reduction). To begin this process, I've conducted PCA on the predictors here. As a note, I elected not to transform the variables since they are all on the same rating scale.

Selecting the correct number of factors here can be accomplished using a scree plot. The visualization shows a definite break at seven components. That said, the first principle component captures 89% of the variance with only 1% increases thereafter. As such, I think taking seven is too many as PCA2 to PCA7 only capsture 6% of the variance. I'm going to carry forward the first three, which still captures a very high 91% of variance. As always, in a business setting this approach would need to be verified with the brand manager to ensure this was reasonable.

```{r brand pca set up and scree}
brand_pca <- princomp(x = cc_brands[,8:38])

data.frame(dev = brand_pca$sdev ^ 2) %>%
  rownames_to_column(var = "PC") %>%
  mutate(variance_exp = round(dev / (sum(dev)), 2),
         vaf = cumsum(variance_exp),
         PC = str_replace(string = PC, pattern = "Comp.", replacement = "PCA"),
         vaf_line = "line") %>%
  slice(1:9) %>%
  ggplot(aes(PC, vaf, group = vaf_line, label = paste0(vaf * 100,"%"))) +
  geom_line(colour = "royalblue2", size = 1.3) +
  geom_label(hjust = 0, nudge_x = 0.15, nudge_y = 0) +
  geom_vline(xintercept = 7, size = 1.3, colour = "darkorange") +
  labs(title = "PCA scree plot- Variance explanation heavily found in PCA1",
       subtitle = "With variance concentration, choosing 3 principle components seems reasonable despite elbow at 7",
       x = NULL)
```

As part of the PCA, I'll also review the component loadings for each variable. This provides insight into what information is captured in each principle component. To do so, I've developed a faceted bar chart for each component. Overall, the factors do lend enough variability so each can be named and interpreted. These can be found below:

####Factor interpretation and naming

##### - PCA1 attributes: Uniformly negative scores
##### - Name: Steady Dislike

##### - PCA2 attributes: Highest positive scores for trustworthy, prestige, emerging leader, easy access, and cool
##### - Name: Brand Perception

##### - PCA3 attributes: Highest positive scores for service and product related features (clear comms, customer service, easy application, etc.). Additionally, low on brand perception valus
##### - Name: Product and Features Utility

```{r brand factor analysis}
as.data.frame(brand_pca$loadings[,1:3]) %>%
  rownames_to_column(var = "variable") %>%
  rename_all(function(x) gsub(pattern = "Comp.", 
                              replacement = "PCA", 
                              x = x)) %>%
  gather(key = "pca", value = "pca_loadings", -variable) %>%
  ggplot(aes(variable, pca_loadings, fill = pca)) +
  geom_col(show.legend = F) +
  facet_wrap(facets = "pca", nrow = 1) +
  geom_hline(yintercept = 0, alpha = .5, size = 1.3, colour = "darkgray") +
  scale_y_continuous(breaks = seq(-1, 1, .2)) +
  coord_flip() +
  labs(title = "Faceted bar charts for factor loadings from each principle component",
       subtitle = "Factor interpretation- PCA1: Steady Dislike, PCA2: Brand Perception, PCA3: Product and Features Utility",
       y = "pca loadings",
       x = NULL)
```

With the new components named, I've put together a train and test split with the factor scores so a linear model can be developed. These are used the same way the initial predictors are so I've also added the preferences ratings back in so the model can be cast using one data set.

The summary shows signficant coefficients for all three variables. The slope coefficients are far more interpretable here owing to the transformation and seem intuitive. Both brand perception and product utility are associated with higher preference ratings while the steady dislike feature is negative. Between brand and product utility, the perception component has a higher slope coefficient as well. The model's R2 is about .47, which is only a slight drop from the initial linear attempt (.49).

```{r brand pca regression}
brand_factors <- brand_pca$scores %>%
  as.data.frame() %>%
  dplyr::select(1:3) %>%
  rename(Steady_Dislike = "Comp.1",
         Brand_Perception = "Comp.2",
         Product_Utility = "Comp.3") %>%
  mutate(preference_rating = cc_brands$preference_rating,
         respondent = cc_brands$respondent)

pca_train <- brand_factors %>%
  filter(!row_number() %in% profiles$holdout)

pca_test <- brand_factors %>%
  filter(row_number() %in% profiles$holdout)

brand_pca_lm <- lm(preference_rating ~ Steady_Dislike + 
                     Brand_Perception + 
                     Product_Utility, 
                   data = pca_train)

summary(brand_pca_lm)
```

While the pca regression helps with coefficient interpretation, it does not enhance the model's predictive capabilities. This is because the predictors are only being compressed and represented in a different form, which means no new information is added. In fact, the pca model has slightly less variance explanation. 

Reviewing the mean absolute error (MAE) for both the train and test still shows poor predictive powers. The test MAE is about 4.4, which means each prediction is off by that many points. The initial preference ratings only have an 11 point scale so this is poor. For example, a perfect 11 might be predicted at 6.6, a value which tells a very different customer brand preference story. Overall, while the pca model was very useful for getting a better understanding of the relationship between brand attribute ratings and preferences, it is not useful for prediction. Here, the inferential component far outweighs the forward looking aspect.

```{r prediction review}
data.frame(
  model = c("train", "test"),
  mae = c(
    mae(actual = pca_train$preference_rating, 
        predicted = brand_pca_lm$fitted.values),
    mae(actual = pca_test$preference_rating, 
        predicted = predict(brand_pca_lm, pca_test))
  )
) %>% custom_kable()
```

***

###Confirmatory Structural Equation Modelling

modelling theorized causal relationships
reviewing numerous paths between variables
Have to have theory in advance so it's mostly confirmatory
exogenous variables are the independent, causal variables
endogenous are the predictors
SEM is about taking a theory and testing if they work
looking for low error
SMC is R2 (squared multiple correlations) for each endogenous variable
models estimated on covariance between all of the model items


lavaan
sempa

```{r SEM modelling}
brand_model <- "
SF =~ easy_access + channel_access + personalize_service + 
  rewards_for_good + easy_application + customer_service + clear_comms + account_infosec +
  quick_app_approval + best_prod_rates + dual_recognition

BP =~ cool + emerging_leader + innovative + prestige + trustworthy

PA =~ my_best_interests + feel_valued + life_easier + reasonable_fees +
  fair_treatment + good_value

BS =~ products_for_bus + bus_cash_flow + responsive_bus_needs +
  business_advisor + understand_bus_probs + solves_bus_probs + bus_rewards_program +
  easy_bus_finance + easy_bus_invest

#overall latent arrangement

PR =~ SF + BS + BP + PA
"

brand_sem <- sem(model = brand_model, 
                 data = cc_brands,
                 meanstructure = T)  

semPaths(object = brand_sem, whatLabels = "std")
```




```{r reviewing sem}
summary(object = brand_sem,
        rsquare = T,
        standardized = T)
```


```{r sem fit measures}
fitMeasures(object = brand_sem, fit.measures = c("cfi", "rmsea", "srmr", "pvalue")) %>%
  as.data.frame() %>%
  rownames_to_column(var = "fit_measure") %>%
  rename(value = ".") %>%
  mutate(value = round(value, 3)) %>%
  custom_kable()
```


***

###References


####1. "Fit	Statistics	commonly	reported	for CFA	and	SEM", https://www.cscu.cornell.edu/news/Handouts/SEM_fit.pdf

***
